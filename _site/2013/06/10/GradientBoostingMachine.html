<h1>函数估计的Boosting框架</h1>

<h2>函数估计基础</h2>

<h3>目标</h3>

<p>函数估计，即从给定训练数据 x->y 和损失函数，来学习一个函数，使得 F(x) 能够最小化与y之间的损失。</p>

<h3>求解方法</h3>

<p>当学习函数空间确定之后，若学习函数由若干参数控制，则函数优化问题便转化为参数优化问题。</p>

<p>某些情况下，当学习函数与损失函数形式较为简单时，模型的最优解可以通过解析的方式求得。</p>

<p>但在一般情况下，逼近问题的解析解难以直接获得，需要使用数值优化算法，即在一个初始猜测基础上逐步修正，得到最终的最优参数。这样所得的<strong>最优解是初始猜测与后续逐步修正的和</strong>。最常用的参数修正方法即是沿负梯度方向进行最速下降。</p>

<h2>Boosting函数估计</h2>

<p>一般情况下，对于复杂问题，很难直接获得非常契合问题领域的学习空间和参数化的学习模型。我们可以采用Boosting的方法，来逐步逼近问题的解。</p>

<p>Boosting方法一般采用加法模型。即每一步寻找一个新的基函数hi和加权系数wi，使得w*h0+… +whi的损失最小。这种策略称之为前向分阶段加法模型(Forward Stagewise Additive Modeling)。</p>

<p>对某些损失函数，比如指数损失函数，可以采用Adaboost等模型，将参数优化转化为样本加权来得到解决。</p>

<p>但对于复杂的损失函数（指数函数鲁棒性不佳），求得一个新的组件的最优参数可能并不容易。此时，可以借助数值优化的思路，不求一次性求得最优解，只求每次寻找到的组件均能给损失带来下降。这便是 Gradient Boosting Machine 的思路。</p>

<p>具体地说，Gradient Boosting Machine 在每一步估算损失函数对当前估计函数的负梯度方向，并对其拟合一个基函数h，使该函数尽可能与负梯度方向平行。然后，沿该函数方向进行线搜索以求得使损失函数取得最大下降的步长w。这样，一个新的Boosting组建w与h便学习得到了。</p>
